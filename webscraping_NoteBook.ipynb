{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Web Scrapping</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>L'importation des bibliothèques</h4>\n",
    "<ul>\n",
    "<li><p>Le module <strong>requests</strong> vous permet d'envoyer des requêtes HTTP en utilisant Python. La requête HTTP renvoie un objet de réponse avec toutes les données de réponse (contenu, encodage, statut, etc.).</p></li>\n",
    "\n",
    "<li><p><strong> Beautiful Soup </strong> est une bibliothèque Python utilisée à des fins de grattage Web pour extraire les données des fichiers HTML et XML. Il crée une arborescence d'analyse à partir du code source de la page qui peut être utilisée pour extraire des données de manière hiérarchique et plus lisible.</p></li>\n",
    "\n",
    "\n",
    "<li><p><strong>Python RegEx.</strong> Une RegEx, ou expression régulière, est une séquence de caractères qui forme un modèle de recherche. RegEx peut être utilisé pour vérifier si une chaîne contient le modèle de recherche spécifié.</p></li>\n",
    "\n",
    "\n",
    "<li><p>Le format dit <strong>CSV</strong> (Comma Separated Values) est le format d'importation et d'exportation le plus courant pour les feuilles de calcul et les bases de données. ... Le module csv implémente des classes pour lire et écrire des données tabulaires au format CSV.</p></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from  bs4 import BeautifulSoup as soup\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Dans cette section, on définit la liste <strong>\"header\"</strong> contenant les 2 \"headers\", puis un ouvre un fichier CSV qu'on nomme <strong>\"hotel_reviews.csv\"</strong> en mode écriture dans une variable <strong>data</strong>. Ensuite, on instancie un <strong>writer</strong> de notre fichier et on choisit la méthode<strong> writerow()</strong> pour écrire dessus.\n",
    "La liste <strong>rating_counter</strong> permet de fixer un seuil pour chaque rating \"1000 par exemple\" et <strong>full</strong> est un Boolean qui est True quand tous les seuils sont égaux à \"1000 par exemple\" .\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Review\", \"Rating\"]\n",
    "\n",
    "data = open(r'scraped_data.csv', 'w', encoding=\"utf-8\", newline=\"\")\n",
    "\n",
    "writer = csv.writer(data)\n",
    "\n",
    "writer.writerow(header)\n",
    "\n",
    "rating_counter = [0,0,0,0,0]\n",
    "full = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>On définit une fonction<strong>scarp_data()</strong> ayant pour objectif l'extraction des données à partir d'un URL quelconque et insérer ces derniers dans notre fichier CSV. Ce lien URL renvoie vers une page qui contient un certain nombre d'avis qu'on va mettre dans un variable <strong>list_data_reviews</strong>. Pour chaque élémentt de cette liste on cherche l'avis ainsi l'évaluation correspondante. Si pour un rating précis on est déjà arrivé au nombre d'occurence de 1000, on continue vers l'itération suivante. Sinon, on extrait les données et on incrémente la valeur correpondante à ce rating dans la liste <strong>rating_counter</strong>.\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_data(url):\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup1 = soup(page.content, \"html.parser\")\n",
    "    soup2 = soup(soup1.prettify(), 'html.parser')\n",
    "\n",
    "    list_data_reviews = soup2.findAll('div', {'class': 'cqoFv _T'})\n",
    "\n",
    "    for data_review in list_data_reviews:\n",
    "        review = data_review.find('q', {'class': 'XllAv H4 _a'})\n",
    "        rating = data_review.find('div', {'class': 'emWez F1'})\n",
    "        rating_value = rating.find('span').attrs['class'][1][7]\n",
    "        if ( rating_counter[int(rating_value)-1] < 5 ):\n",
    "            row = [review.find('span').text.strip(), rating_value]\n",
    "            rating_counter[int(rating_value) - 1] += 1\n",
    "            writer.writerow(row)\n",
    "\n",
    "            print(rating_counter)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>On définit une fontcion <strong>number_reviews()</strong> qui détermine le nombre d'avis sur une page </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_reviews(url):\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup1 = soup(page.content, \"html.parser\")\n",
    "    soup2 = soup(soup1.prettify(), 'html.parser')\n",
    "\n",
    "    list_data_reviews = soup2.findAll('div', {'class': 'cqoFv _T'})\n",
    "    return len(list_data_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>On définit une fonction <strong>last_page()</strong> qui détermine si la page actuelle est la dernière ou pas.</li>\n",
    "</ul>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_page(url):\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup1 = soup(page.content, \"html.parser\")\n",
    "    soup2 = soup(soup1.prettify(), 'html.parser')\n",
    "    pages = soup2.find('div', {'class':'pageNumbers'})\n",
    "    current_page = pages.find('span', {'class':'pageNum current disabled'})\n",
    "    current_page_num = int(current_page.text.strip())\n",
    "\n",
    "    other_pages = pages.findAll('a',{'class':'pageNum'})\n",
    "    list_other_pages = []\n",
    "    for other_page in other_pages:\n",
    "        list_other_pages.append(int(other_page.text.strip()))\n",
    "    max_page = max(list_other_pages)\n",
    "\n",
    "    if current_page_num < max_page :\n",
    "        return False\n",
    "    else :\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<ul>\n",
    "<li>Maintenant, on cherche à récuperer tous les hotels se trouvant sur la page principale. On s'intéresse essentiellement aux hotels de la région Parisienne.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'}\n",
    "\n",
    "url = \"https://www.tripadvisor.com/Hotels-g187147-Paris_Ile_de_France-Hotels.html\"\n",
    "\n",
    "page = requests.get(url, headers=headers)\n",
    "soup1 = soup(page.content, \"html.parser\")\n",
    "soup2 = soup(soup1.prettify(),'html.parser')\n",
    "\n",
    "list_hotels = soup2.find('div', {'class':'relWrap'})\n",
    "hotels = list_hotels.findAll('div', {'class':'prw_rup prw_meta_hsx_responsive_listing ui_section listItem'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Enfin !</h3>\n",
    "<ul>\n",
    "<li>Pour chaque hotel, on extrait les avis et les évalautions tant que laliste  <strong>rating_counter</strong> n'est pas pleine. On parcourt chaque page à l'aide de la fonction<strong> number_reviews()</strong>  dont on insère le résultat au niveau de URL d'origine et commence on parcourt tt les pages pour chaque hotel.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for hotel in hotels:\n",
    "\n",
    "    if ( not full )\n",
    "    \n",
    "        link = hotel.find('a', {'class': 'property_title prominent'}, href=True)\n",
    "        if (link):\n",
    "\n",
    "            url_origin = \"https://www.tripadvisor.com\" + link['href']\n",
    "            url = url_origin\n",
    "            scrap_data(url)\n",
    "            cmp = 0\n",
    "            \n",
    "            while (not last_page(url) and not full ):\n",
    "\n",
    "                cmp += number_reviews(url)\n",
    "                url = re.sub(r'(Reviews-)', \"or\" + str(cmp) + \"-\", url_origin)\n",
    "                scrap_data(url)\n",
    "                if ( rating_counter[0] == 5 and rating_counter[1] == 5 and rating_counter[2] == 5 and rating_counter[3] == 5 and\n",
    "                    rating_counter[4] == 5 ):\n",
    "                    full = True\n",
    "\n",
    "    else:\n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
